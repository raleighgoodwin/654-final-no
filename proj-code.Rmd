---
title: "proj-code"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r seed}
set.seed(1262023)
```

# prep data
```{r import data}
# libraries
library(tidyverse)
library(rio)
library(here)

# import main data set
data <- import(here("data", "scored-data.csv"))
# name id column for joining
data <- data %>% 
  select(-id)
colnames(data)[1] = "id" 

# import sentence embeddings data sets
open1 <- import(here("data", "open1_embed.csv"))
studying1 <- import(here("data", "studying1_embed.csv"))
studying2 <- import(here("data", "studying2_embed.csv"))
# name id columns for joining
colnames(open1)[1] = "id" 
colnames(studying1)[1] = "id"
colnames(studying2)[1] = "id"
```

```{r fix colnames}
# fix colnames so they join properly
open1lab <- c("id", rep(NA, length(open1)-1))
studying1lab <- c("id", rep(NA, length(studying1)-1))
studying2lab <- c("id", rep(NA, length(studying2)-1))

og_cols <- colnames(open1)

for (i in 2:length(open1)) {
  
  open1lab[i] = paste(og_cols[i], "o1", sep = "_")
  studying1lab[i] = paste(og_cols[i], "s1", sep = "_")
  studying2lab[i] = paste(og_cols[i], "s2", sep = "_")

}

colnames(open1) <- open1lab
colnames(studying1) <- studying1lab
colnames(studying2) <- studying2lab
```

```{r load libraries}
library(tidyverse)
library(dplyr)
library(caret)
library(recipes)
library(tm)
library(glmnet)
library(rpart)
```

# select data variables
```{r excl var}
# get rid of cols with NA values
data1 <- data[ , colSums(is.na(data))==0]

# get rid of irrelevant/autogenerated columns in original df
data2 <- data1 %>% 
  select(-contains("batch"), -contains("_date_"), -contains("status_"), 
         -contains("ip_address_"),
         -contains("progress_"), -contains("recipient_"),
         -contains("duration_"), 
         -contains("finished_"), -contains("response_id_"), 
         -contains("external_reference_"), -contains("location_"),
         -contains("distribution_channel_"), -contains("user_language_"), 
         -contains("consent"), -contains("time_"), -contains("unique_code_"),
         -contains("_click_"), -contains("page_submit"), -contains("meta"),
         -contains("cheat"), -contains("attn_check")
         )
```

```{r select var}
# select relevant variables from original df
data3 <- data2 %>% 
  select(id, 
         dem_age_b, dem_ed_b, gender_b, race_b, conservatism_f, con, party, 
         contains("sc_"), contains("rav"), contains("aot"), contains("nfc"),
         contains("vl"), contains("_ch"), 
         cc_risk_sc,
         -contains("scoredz"), -contains("_chz"), -contains("zscored"),
         -contains("training"), -contains("ravens")
         )
```

```{r join dfs}
# join main df & sentence embeddings dfs
data4 <- left_join(data3, open1, by = "id")
data5 <- left_join(data4, studying1, by = "id")
data6 <- left_join(data5, studying2, by = "id") 

df <- data6 %>% 
  select(cc_risk_sc, everything(), -id)
```

taking out variables w/ low variability and multicollinearity b/c was having issues w/ model fit

```{r df1}
df1 <- df %>% 
  select(dem_age_b, dem_ed_b, gender_b, race_b, conservatism_f, 
         sc_scored, rav_scored,
         aot_scored, nfc_scored,
         vl_scored, contains("_ch"), 
         cc_risk_sc,
         -contains("scoredz"), -contains("_chz"), -contains("zscored"),
         -contains("training"), -contains("ravens"), 
         contains("o1"), contains("s1"), contains("s2")
         )

write.csv(df1, file=here::here("data","df1.csv"), row.names = FALSE)
```

# blueprint, tr/te, cv
```{r variable roles}
outcomeV <- "cc_risk_sc"
df1$cc_risk_sc <- df$cc_risk_sc %>%  as.numeric()

# categorical variables
catV <- df1 %>% 
  select(gender_b, race_b
         ) 

catVlab <- catV %>% 
  colnames()

df1[catVlab] <- lapply(df1[catVlab], as.factor)

# continuous variables
contV <- df1 %>% 
  select(dem_age_b, dem_ed_b, conservatism_f, sc_scored,
         rav_scored, 
         aot_scored, nfc_scored,
         vl_scored, contains("_ch"), cc_risk_sc
         ) 

contVlab <- contV %>% 
  colnames()

df1[contVlab] <- lapply(df1[contVlab], as.numeric)

# sentence embeddings variables
stV <- df1 %>% 
  select(contains("o1"), contains("s1"), contains("s2")) %>% 
  colnames()

stVlab <- stV %>% 
  colnames()

df1[stVlab] <- lapply(df1[stVlab], as.numeric)

# predictors info
predV <- c(catVlab, contVlab, stVlab)
predN <- ncol(df1)-1
```

```{r blueprint}
blueprint <- recipe(x = df1,
                          vars  = colnames(df1),
                          roles = c('outcome',rep('predictor', predN))) %>% 
  step_normalize(c(contVlab, stVlab)) %>% 
  step_dummy(c(gender_b, race_b), one_hot = TRUE)
```

```{r tr/te sets}
sample_tr <- sample(1:nrow(df1), round(nrow(df1) * 0.8)) # randomly split data

df_tr  <- df1[sample_tr, ] # create training and test sets

df_te  <- df1[-sample_tr, ]
```

```{r 10folds}
tr_r <- df_tr[sample(nrow(df_tr)),] # randomize row order in training set

tr_folds <- cut(seq(1,nrow(tr_r)),breaks=10,labels=FALSE)

fold_indices <- vector('list',10)

for(i in 1:10){
  fold_indices[[i]] <- which(tr_folds!=i)
}

tr_cv <- trainControl(method = "cv",
                             index  = fold_indices)
```

# output table df for model comparison
```{r output table}
# create somewhere to save output
perf_tab <- data.frame(Model = c('Linear Regression', 
                                 'Linear Regression with Ridge Penalty',
                                'Linear Regression with Elastic Net Penalty',
                                'Decision Trees',
                                'Random Forests'), 
                          Rsq = rep(NA,5), 
                          MAE = rep(NA,5),
                          RMSE = rep(NA,5)
                         )
```

# linear regression model
```{r train model}
lmod <- caret::train(blueprint,
                          data      = df_tr,
                          method    = "lm",
                          trControl = tr_cv)
```

```{r model te perf}
lm_pred <- predict(lmod, df_te)

# R^2
rsq_lm <- cor(df_te$cc_risk_sc,lm_pred)^2
perf_tab$Rsq[1] <- rsq_lm %>% round(5)

# MAE
mae_lm <- mean(abs(df_te$cc_risk_sc - lm_pred))
perf_tab$MAE[1] <- mae_lm %>% round(5)

# RMSE
rmse_lm <- sqrt(mean((df_te$cc_risk_sc - lm_pred)^2))
perf_tab$RMSE[1] <- rmse_lm %>% round(5)

perf_tab
```

# linear regression w/ ridge penalty
```{r grid}
# Create the tuning grid
rg_grid <- expand.grid(alpha = 0, lambda = seq(11,12,.005))
  
rg_grid
```

```{r train model}
ridge <- caret::train(blueprint, 
                        data      = df_tr, 
                        method    = "glmnet", 
                        trControl = tr_cv,
                        tuneGrid  = rg_grid)
```

```{r best tune}
ridge$bestTune
plot(ridge)
```

```{r model te perf}
rg_pred <- predict(ridge, df_te)

# R^2
rsq_rg <- cor(df_te$cc_risk_sc,rg_pred)^2
perf_tab$Rsq[2] <- rsq_rg %>% round(5)

# MAE
mae_rg <- mean(abs(df_te$cc_risk_sc - rg_pred))
perf_tab$MAE[2] <- mae_rg %>% round(5)

# RMSE
rmse_rg <- sqrt(mean((df_te$cc_risk_sc - rg_pred)^2))
perf_tab$RMSE[2] <- rmse_rg %>% round(5)

perf_tab
```

# linear regression w/ elastic net penalty
```{r grid}
# Create the tuning grid
el_grid <- expand.grid(alpha = seq(0,.1,.005), lambda = seq(0,20,2.5))
  
el_grid
```

```{r train/tune model}
elastic <- caret::train(blueprint, 
                        data      = df_tr, 
                        method    = "glmnet", 
                        trControl = tr_cv,
                        tuneGrid  = el_grid)

elastic$bestTune
plot(elastic)
```

```{r model te perf}
el_pred <- predict(elastic, df_te)

# R^2
rsq_el <- cor(df_te$cc_risk_sc,el_pred)^2
perf_tab$Rsq[3] <- rsq_el %>% round(5)

# MAE
mae_el <- mean(abs(df_te$cc_risk_sc - el_pred))
perf_tab$MAE[3] <- mae_el %>% round(5)

# RMSE
rmse_el <- sqrt(mean((df_te$cc_risk_sc - el_pred)^2))
perf_tab$RMSE[3] <- rmse_el %>% round(5)

perf_tab
```

# decision trees
```{r cp grid}
dt_grid <- data.frame(cp=seq(0,100,1))
```

```{r fixed train/tune}
# dt_rpart <- caret::train(blueprint,
#                             data      = df_tr,
#                             method    = 'rpart',
#                             tuneGrid  = dt_grid,
#                             trControl = tr_cv,
#                             control   = list(minsplit = 20,
#                                              minbucket = 2,
#                                              maxdepth = 60))
```

```{r train/tune p1}
maxdepth <- seq(40, 70, 10)

minsplit <- seq(10, 30, 10)

repnum <- length(maxdepth) * length(minsplit)

dt_hps <- data.frame(split = unlist(lapply(minsplit, rep, length(maxdepth))),
                     depth = rep(maxdepth, length(minsplit)))

dt_mods <- list(NA)

for(i in 1:nrow(dt_hps)){

  dt_mods[[i]] <- NA

}
```

```{r train/tune p2}
for(i in 1:length(dt_mods)){
    
dt_mods[[i]] <- caret::train(blueprint,
                            data      = df_tr,
                            method    = 'rpart',
                            tuneGrid  = dt_grid,
                            trControl = tr_cv,
                            control   = list(minsplit = dt_hps[i,1],
                                             minbucket = 2,
                                             maxdepth = dt_hps[i,2]))
}
```

```{r best tune}
tune <- list(NA)

for (i in 1:length(dt_mods)) {
  tune[[i]] = dt_mods[[i]]$bestTune
}

lapply(dt_mods, plot)
```

```{r model te perf}
# dt_pred <- predict(dt_rpart, df_te)
# 
# # R^2
# rsq_dt <- cor(df_te$cc_risk_sc,dt_pred)^2
# perf_tab$Rsq[4] <- rsq_dt %>% round(5)
# 
# # MAE
# mae_dt <- mean(abs(df_te$cc_risk_sc - dt_pred))
# perf_tab$MAE[4] <- mae_dt %>% round(5)
# 
# # RMSE
# rmse_dt <- sqrt(mean((df_te$cc_risk_sc - dt_pred)^2))
# perf_tab$RMSE[4] <- rmse_dt %>% round(5)
# 
# perf_tab

# dt_coefs <- list(rep(NA,3), rep(NA,3), rep(NA,3), rep(NA,3), rep(NA,3), 
#                   rep(NA,3), rep(NA,3), rep(NA,3), rep(NA,3))
```

# random forests
```{r}

```

